import sqlite3

import streamlit as st
import tempfile
import os
from PIL import Image
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
conn = sqlite3.connect('pdf_vector_data.db')
cursor = conn.cursor()

# ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìƒì„± (í…Œì´ë¸” ìƒì„±)
cursor.execute('''CREATE TABLE IF NOT EXISTS pdf_vectors (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    page_number INTEGER,
                    vector BLOB
                 )''')
conn.commit()


#íŒŒì¼ ì—…ë¡œë“œ
# ["samsung_tv_manual.pdf", "lg_ac_manual.pdf", "winix_humidifier_manual.pdf"]
tv_file = PyPDFLoader("samsung_tv_manual.pdf")
ac_file = PyPDFLoader("lg_ac_manual.pdf")
hm_file = PyPDFLoader("winix_humidifier_manual.pdf")

def document_to_db(uploaded_file, size):    # ë¬¸ì„œ í¬ê¸°ì— ë§ê²Œ ì‚¬ì´ì¦ˆ ì§€ì • --> size
    pages = uploaded_file.load_and_split()
    #Split
    text_splitter = RecursiveCharacterTextSplitter(
        # Set a really small chunk size, just to show.
        chunk_size = size,
        chunk_overlap  = 20,
        length_function = len,
        is_separator_regex = False,
    )
    texts = text_splitter.split_documents(pages)

    #Embedding
    embeddings_model = OpenAIEmbeddings()

    # load it into Chroma
    db = Chroma.from_documents(texts, embeddings_model)
    return db
'''
#ì—…ë¡œë“œ ë˜ë©´ ë™ì‘í•˜ëŠ” ì½”ë“œ
if uploaded_file is not None:
    pages = pdf_to_document(uploaded_file)

    #Split
    text_splitter = RecursiveCharacterTextSplitter(
        # Set a really small chunk size, just to show.
        chunk_size = 300,
        chunk_overlap  = 20,
        length_function = len,
        is_separator_regex = False,
    )
    texts = text_splitter.split_documents(pages)

    #Embedding
    embeddings_model = OpenAIEmbeddings()

    persist_directory = 'db'
    # load it into Chroma
    db = Chroma.from_documents(documents=texts,
                                 embedding=embeddings_model,
                                 persist_directory=persist_directory)
    db.persist()
    db = None
    db = Chroma(persist_directory=persist_directory,
                  embedding_function=embeddings_model)
'''
#st.balloons()

#ì œëª©
st.title("SightnSpeak")
st.write("---")

with st.sidebar:
    openai_api_key = st.text_input("OpenAI API Key", type="password")

# ì´ˆê¸° ì„¸ì…˜ ìƒíƒœ ì„¤ì •
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = {'AC': [], 'TV': [], 'HM': []}

    
# ë©”ë‰´ ì„ íƒ
selected_option = st.selectbox('ì„ íƒí•  ê¸°ê¸°ë¥¼ ë°”ë¼ë³´ì„¸ìš”', ['TV', 'ê°€ìŠµê¸°', 'ì—ì–´ì»¨'])


# ì‚¬ìš©ìê°€ ì„ íƒí•œ ì˜µì…˜ì— ë”°ë¼ ë‹¤ë¥¸ ì½˜í…ì¸  í‘œì‹œ
if selected_option == 'TV':
  db_tv = document_to_db(tv_file, 500)

  tv_img = Image.open('person_TV.jpg')
  tv_img = tv_img.resize((100, 100))
  st.image(tv_img)
  
  st.success('ë‹¹ì‹ ì€ TVë¥¼ ë°”ë¼ë³´ê³  ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤!')
  st.header('TV :sunglasses:',divider='rainbow')

  tv_question = st.text_input('TVì—ê²Œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”')

  tv_question = st.text_input('TVì—ê²Œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”')
  if st.button('TVì—ê²Œ ì§ˆë¬¸í•˜ê¸°', key='tv_button'):
      with st.spinner('Wait for it...'):
          llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
          qa_chain = RetrievalQA.from_chain_type(llm, retriever=db_tv.as_retriever())
          result = qa_chain({"query": tv_question})
          st.session_state.chat_history['TV'].append({"question": tv_question, "answer": result["result"]})

  # ì±— ê¸°ë¡ ì¶œë ¥
  for chat in st.session_state.chat_history['TV']:
      st.text(f"ğŸ¤” {wrap_text(chat['question'])}")
      st.text(f"ğŸ˜Š {wrap_text(chat['answer'])}")
      st.write("---")

elif selected_option == 'ê°€ìŠµê¸°':
  db_hm = document_to_db(hm_file, 300)

  st.success('ë‹¹ì‹ ì€ ê°€ìŠµê¸°ë¥¼ ë°”ë¼ë³´ê³  ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤!')
  st.header('ê°€ìŠµê¸° :sunglasses:',divider='rainbow')

  hm_question = st.text_input('ê°€ìŠµê¸°ì—ê²Œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”', key='hm')
  if st.button('ê°€ìŠµê¸°ì—ê²Œ ì§ˆë¬¸í•˜ê¸°'):
      with st.spinner('Wait for it...'):
          llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
          qa_chain = RetrievalQA.from_chain_type(llm, retriever=db_hm.as_retriever())
          result = qa_chain({"query": hm_question})
          st.session_state.chat_history['HM'].append({"question": hm_question, "answer": result["result"]})

  # ì±— ê¸°ë¡ ì¶œë ¥
  for chat in st.session_state.chat_history['HM']:
      st.text(f"ğŸ¤” {wrap_text(chat['question'])}")
      st.text(f"ğŸ˜Š {wrap_text(chat['answer'])}")
      st.write("---")

elif selected_option == 'ì—ì–´ì»¨':
  db_ac = document_to_db(ac_file, 500)

  st.success('ë‹¹ì‹ ì€ ì—ì–´ì»¨ì„ ë°”ë¼ë³´ê³  ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤!')
  st.header('ì—ì–´ì»¨ :sunglasses:',divider='rainbow')

  ac_question = st.text_input('ì—ì–´ì»¨ì—ê²Œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”', key='ac')
  if st.button('ì—ì–´ì»¨ì—ê²Œ ì§ˆë¬¸í•˜ê¸°'):
      with st.spinner('Wait for it...'):
          llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
          qa_chain = RetrievalQA.from_chain_type(llm, retriever=db_ac.as_retriever())
          result = qa_chain({"query": ac_question})
          st.session_state.chat_history['AC'].append({"question": ac_question, "answer": result["result"]})

  # ì±— ê¸°ë¡ ì¶œë ¥
  for chat in st.session_state.chat_history['AC']:
      st.text(f"ğŸ¤” {wrap_text(chat['question'])}")
      st.text(f"ğŸ˜Š {wrap_text(chat['answer'])}")
      st.write("---")


   
